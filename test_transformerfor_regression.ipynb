{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a642e8dc-da93-471d-8fce-aa030c68c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utils import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "db3e9d74-7cc9-4b04-b988-f6df8fd03e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1716, 0.7469, 0.4589, 0.9850]]])\n",
      "tensor([[[-0.9002,  0.5805, -1.0237,  1.3434]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-1.3024,  0.0461, -0.2415,  1.4977],\n",
      "         [-1.0205,  0.5959, -0.9087,  1.3333]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9002,  0.5805, -1.0237,  1.3434]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = nn.TransformerDecoderLayer(d_model=4, nhead=2, dim_feedforward=50, \n",
    "                                 batch_first=True, dropout=0.0)\n",
    "obs_enc = MLP(2,10,output_size=4, unsqueeze=False)\n",
    "dec_mu = MLP(1,12,input_size=10, \n",
    "             last_activation=lambda y: nn.functional.tanh(y)*10,\n",
    "             unsqueeze=False)\n",
    "dec_sigma = MLP(1,12,input_size=10, \n",
    "                last_activation=torch.exp, \n",
    "                unsqueeze=False)\n",
    "dec_mixture = MLP(1,12,input_size=10, \n",
    "                  last_activation=torch.exp, \n",
    "                  unsqueeze=False)\n",
    "obs = torch.rand(1,1,1)\n",
    "start = torch.rand(1,1,4)\n",
    "\n",
    "obs_embed = obs_enc(obs)\n",
    "start.expand(obs_embed.shape).repeat(1,2,1)\n",
    "print(start)\n",
    "output_1 = dec(start.expand(obs_embed.shape), obs_embed)\n",
    "print(output_1)\n",
    "output_2 = dec(torch.cat([start, output_1[:,-1:,:].detach()], dim=-2), obs_embed)\n",
    "print(output_2)\n",
    "output_1[:,-1:,:].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b14eaf4-75fe-4840-96a4-61248d094869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 2, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = torch.rand(11,1,10)\n",
    "test = torch.rand(11,1,10)\n",
    "torch.cat([obs,test], dim=-2).shape\n",
    "# enc_obs = obs_enc(obs).unsqueeze(1)\n",
    "# # enc_obs.shape = torch.Size([11, 1, 10])\n",
    "\n",
    "# x = torch.rand(11,1,10) # batch, seq=1 '<start>', features\n",
    "# dec_emb = dec(x,enc_obs) #shape = 11, 1 , 10\n",
    "# dec_mu(dec_emb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d721168d-b8b9-4cde-92e3-4bcb0fccd653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqGaussMixPosterior(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed_obs = MLP(2,10,output_size=10, unsqueeze=False)\n",
    "        self.decode_mu = MLP(1,12,input_size=10, \n",
    "                             last_activation=lambda y: torch.tanh(y)*10,\n",
    "                             unsqueeze=False)\n",
    "        self.decode_sigma = MLP(1,12,input_size=10, \n",
    "                                last_activation=torch.exp, \n",
    "                                unsqueeze=False)\n",
    "        self.decode_mixture_prob = MLP(1,12,input_size=10, \n",
    "                                       last_activation=torch.exp, \n",
    "                                       unsqueeze=False)\n",
    "        self.trans_dec = nn.TransformerDecoderLayer(d_model=10, nhead=2, dim_feedforward=20, \n",
    "                                                    batch_first=True, dropout=0.0)\n",
    "        \n",
    "        self.start = torch.nn.Parameter(torch.rand(1,1,10))\n",
    "    def forward(self,obs):\n",
    "        obs_embed = self.embed_obs(obs)\n",
    "        so_far_decoded = self.start.expand(obs_embed.shape)\n",
    "        all_mu, all_sigma, all_mix_p =  [], [], []\n",
    "        for _ in range(2):\n",
    "            obs_dec = self.trans_dec(so_far_decoded, obs_embed)\n",
    "            so_far_decoded = torch.cat([so_far_decoded, obs_dec[:,-1:,:]], dim=-2).detach()\n",
    "            all_mu.append(self.decode_mu(obs_dec[:,-1:,:]))\n",
    "            all_sigma.append(self.decode_sigma(obs_dec[:,-1:,:]))\n",
    "            all_mix_p.append(self.decode_mixture_prob(obs_dec[:,-1:,:]))\n",
    "        return {'mixture_probs': torch.cat(all_mix_p, dim=1),\n",
    "                'mus': torch.cat(all_mu, dim=1),\n",
    "                'sigmas': torch.cat(all_sigma, dim=1)}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c6b98505-49e7-4ec5-8f36-b26b3b2e41c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqGaussMixPosterior(\n",
       "  (embed_obs): MLP(\n",
       "    (network): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=1, out_features=10, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode_mu): MLP(\n",
       "    (network): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=10, out_features=12, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=12, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode_sigma): MLP(\n",
       "    (network): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=10, out_features=12, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=12, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode_mixture_prob): MLP(\n",
       "    (network): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=10, out_features=12, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=12, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans_dec): TransformerDecoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=10, out_features=20, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (linear2): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm3): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.0, inplace=False)\n",
       "    (dropout2): Dropout(p=0.0, inplace=False)\n",
       "    (dropout3): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_z = SeqGaussMixPosterior()\n",
    "obs = torch.rand(2,1,1)\n",
    "out = q_z(obs)['mixture_probs']\n",
    "out\n",
    "\n",
    "q_z\n",
    "\n",
    "# all_mu, all_sigma, all_mix_p = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b9d029d2-808b-4e90-98a2-f689e13da7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch():\n",
    "    obs = torch.randn(100,1,1)\n",
    "    less_than_0 = obs<=0\n",
    "    mu_1 = torch.ones(100,1,1) * -1\n",
    "    mu_1[less_than_0] = 1.0\n",
    "    mu_2 = torch.ones(100,1,1) * -2\n",
    "    mu_2[less_than_0] = 2.0\n",
    "    sigma = torch.ones(100,2,1)\n",
    "    mix_p_1 = torch.ones(100,1,1) * 0.6\n",
    "    mix_p_1[less_than_0] = 0.4\n",
    "    mix_p_2 = torch.ones(100,1,1) * 0.7\n",
    "    mix_p_2[less_than_0] = 0.3\n",
    "    return obs, torch.cat([mu_1,mu_2],dim=1), sigma, torch.cat([mix_p_1,mix_p_2],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "022a5192-423a-4fc9-86da-a96d9185e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_z_given_obs = SeqGaussMixPosterior()\n",
    "num_iterations = 1000\n",
    "optim = torch.optim.Adam(q_z_given_obs.parameters())\n",
    "losses = []\n",
    "for _ in range(num_iterations):\n",
    "    obs, mu, sigma, mix_p = gen_batch()\n",
    "    out = q_z_given_obs(obs)\n",
    "    loss_mu = nn.functional.mse_loss(out['mus'], mu)\n",
    "    loss_p = nn.functional.mse_loss(out['mixture_probs'], mix_p)\n",
    "    loss_sig = nn.functional.mse_loss(out['sigmas'], sigma)\n",
    "    loss = loss_mu + loss_p + loss_sig\n",
    "    losses.append(loss.detach())\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c697f423-9426-49ee-b41c-cf97e302570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mixture_probs': tensor([[[0.6144],\n",
      "         [0.6933]]], grad_fn=<CatBackward0>), 'mus': tensor([[[-1.0640],\n",
      "         [-2.0548]]], grad_fn=<CatBackward0>), 'sigmas': tensor([[[0.9997],\n",
      "         [0.9975]]], grad_fn=<CatBackward0>)}\n",
      "{'mixture_probs': tensor([[[0.3534],\n",
      "         [0.3280]]], grad_fn=<CatBackward0>), 'mus': tensor([[[0.9773],\n",
      "         [2.0022]]], grad_fn=<CatBackward0>), 'sigmas': tensor([[[1.0018],\n",
      "         [1.0028]]], grad_fn=<CatBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "#training was successful on this toy example\n",
    "print(q_z_given_obs(torch.tensor([[[1.0]]])))\n",
    "print(q_z_given_obs(torch.tensor([[[-1.0]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1252f5d-5762-47a8-8668-aeea24c061ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
